{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes through the steps used to begin exploring clean the raw data obtained. Before we can clean the data, it is important to understand the overall properties of the data. This can help identify data that is important to our goals and data that is unnecessary for training our desired model. Cleaning makes sure the tweets are in the best format for analysis and training a model for sentiment analysis.\n",
    "\n",
    "__Columns__\n",
    "- `target` sentiment of tweet (0=positive, 4=negative)\n",
    "- `id` unique identifier of tweet\n",
    "- `date` date and time of tweet\n",
    "- `flag` the query if one exists\n",
    "- `user` user that authored the tweet\n",
    "- `text` text content of the tweet, emoticons are removed\n",
    "\n",
    "This data is publicly available on [Kaggle](https://www.kaggle.com/kazanova/sentiment140) and was created by the authors of the publication:\n",
    "\n",
    "    Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12.\n",
    "\n",
    "_Note: \n",
    "This notebook expects the zipped dataset to be in the `./resources` sub-directory with the name `raw_twitter_data.zip`\n",
    "(e.g., `./resources/raw_twitter_data.zip`). If the dataset does not exist it will be downloaded and saved in this location._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\developer\\lib\\anaconda3\\lib\\site-packages (1.19.2)\n",
      "Requirement already satisfied: pandas in c:\\developer\\lib\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: tokenization in c:\\developer\\lib\\anaconda3\\lib\\site-packages (1.0.7)\n",
      "Requirement already satisfied: regex in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from tokenization) (2020.10.15)\n",
      "Requirement already satisfied: wordcloud in c:\\developer\\lib\\anaconda3\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: pillow in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from wordcloud) (8.0.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from wordcloud) (1.19.2)\n",
      "Requirement already satisfied: matplotlib in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from wordcloud) (3.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2020.6.20)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.0)\n",
      "Requirement already satisfied: six in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install tokenization\n",
    "!{sys.executable} -m pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tokenization\n",
    "\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read File\n",
    "Read the file into memory to determine its overall shape. If the file does not exist locally it is downloaded into the `./resources` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'raw_twitter_data.zip'\n",
    "resources_dir = 'resources'\n",
    "file_path = './{}/{}'.format(resources_dir, file_name)\n",
    "is_file_downloaded = os.path.exists(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_file_downloaded:\n",
    "    if not os.path.exists(resources_dir):\n",
    "        os.makedirs(resources_dir)\n",
    "    \n",
    "    file_url = 'https://dorman-public-data.s3-us-west-2.amazonaws.com/' + file_name\n",
    "    req = Request(file_url)\n",
    "    \n",
    "    with open(file_path, 'wb') as target:\n",
    "        target.write(urlopen(req).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "df = pd.read_csv(file_path, encoding = 'ISO-8859-1', names=col_names, compression='zip')\n",
    "df = df.dropna(how=\"any\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def df_summary(data_frame):\n",
    "    BYTES_IN_MB = 1024**2\n",
    "    print('Columns: {}'.format(data_frame.columns))\n",
    "    print('Shape: {}'.format(data_frame.shape))\n",
    "    print('Memory Usage:{:.4f} Mb'.format(data_frame.memory_usage().sum() / BYTES_IN_MB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: Index(['target', 'id', 'date', 'flag', 'user', 'text'], dtype='object')\n",
      "Shape: (1600000, 6)\n",
      "Memory Usage:73.2423 Mb\n"
     ]
    }
   ],
   "source": [
    "df_summary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag             user                                               text\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton  is upset that he can't update his Facebook by ...\n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus  @Kenichan I dived many times for the ball. Man...\n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF    my whole body feels itchy and like its on fire \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Extra Columns\n",
    "The only columns of real interest to train a model for tweet sentiment classification is the `target` and `text` columns. The `target` column indicates the sentiment of the tweet and `text` contains the text of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: Index(['target', 'text'], dtype='object')\n",
      "Shape: (1600000, 2)\n",
      "Memory Usage:24.4142 Mb\n"
     ]
    }
   ],
   "source": [
    "df.drop(['id', 'date', 'flag', 'user'], axis='columns', inplace=True)\n",
    "df_summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis\n",
    "Examining the data for differences in data points associated with different sentiments can help us understand if our data is balanced across our target categories. Additionally, it can helps use understand if we should direct the focus of our training on to or away from any data characteristics to get the best possible sentiment classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_words'] = df['text'].apply(lambda s: len(s.split(' ')))\n",
    "df['num_unique_words'] = df['text'].apply(lambda s: len(set(s.split(' '))))\n",
    "df['num_stop_words'] = df['text'].apply(lambda s: len([w for w in s.lower().split(' ') if w in STOPWORDS]))\n",
    "df['avg_word_length'] = df['text'].apply(lambda s: np.mean([len(w) for w in s.split(' ')]))\n",
    "df['num_chars'] = df['text'].apply(lambda s: len(s))\n",
    "df['num_punctuation'] = df['text'].apply(lambda s: len([c for c in s if c in string.punctuation]))\n",
    "df['num_urls'] = df['text'].apply(lambda s: len([w for w in s.lower().split() if 'http' in w or 'https' in w]))\n",
    "df['num_hashtags'] = df['text'].apply(lambda s: len([c for c in s if c == '#']))\n",
    "df['num_mentions'] = df['text'].apply(lambda s: len([c for c in s if c == '@']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "meta_feature_cols = ['num_words', 'num_unique_words', 'num_stop_words', 'avg_word_length', \n",
    "                     'num_chars', 'num_punctuation', 'num_urls', 'num_hashtags', 'num_mentions']\n",
    "positive_tweets = df['target'] == 0\n",
    "\n",
    "figure, axes = plt.subplots(ncols=1, nrows=len(meta_feature_cols), figsize=(10,25), dpi=100)\n",
    "\n",
    "for i, meta_feature in enumerate(meta_feature_cols):\n",
    "    sns.histplot(df.loc[positive_tweets][meta_feature], kde=True, label=\"Positive Tweets\", ax=axes[i])\n",
    "    sns.histplot(df.loc[~positive_tweets][meta_feature], kde=True, label=\"Negative Tweets\", ax=axes[i], color=\"red\")\n",
    "    \n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].tick_params(axis='x', labelsize=8)\n",
    "    axes[i].tick_params(axis='y', labelsize=8)\n",
    "    axes[i].legend()\n",
    "    \n",
    "    axes[i].set_title('{} Target Distribution'.format(meta_feature), fontsize=8)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta plots for each series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([800000, 800000], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_balance = df.groupby('target')['target'].agg('count').values\n",
    "target_balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
