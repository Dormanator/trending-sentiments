{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 08 22:02:42 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 456.71       Driver Version: 456.71       CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 106... WDDM  | 00000000:02:00.0  On |                  N/A |\n",
      "|  0%   35C    P8     6W / 120W |    402MiB /  6144MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       476    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      3572    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      3876    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      3984    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A      4484    C+G   ...b3d8bbwe\\WinStore.App.exe    N/A      |\n",
      "|    0   N/A  N/A      5196    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      5464    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      6044    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A      6848    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      7332    C+G   ...zpdnekdrzrea0\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A      7376    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A      7612    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A      9460    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A      9588    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\n",
      "|    0   N/A  N/A     12248    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes through the steps used to begin exploring clean the raw data obtained. Before we can clean the data, it is important to understand the overall properties of the data. This can help identify data that is important to our goals and data that is unnecessary for training our desired model. Cleaning makes sure the tweets are in the best format for analysis and training a model for sentiment analysis.\n",
    "\n",
    "__Columns__\n",
    "- `target` sentiment of tweet (0=negative, 4=positive)\n",
    "- `id` unique identifier of tweet\n",
    "- `date` date and time of tweet\n",
    "- `flag` the query if one exists\n",
    "- `user` user that authored the tweet\n",
    "- `text` text content of the tweet, emoticons are removed\n",
    "\n",
    "This data is publicly available on [Kaggle](https://www.kaggle.com/kazanova/sentiment140) and was created by the authors of the publication:\n",
    "\n",
    "    Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12.\n",
    "\n",
    "_Note:_ \n",
    "_This notebook expects the zipped dataset to be in the `./resources` sub-directory with the name `raw_twitter_data.zip`\n",
    "(e.g., `./resources/raw_twitter_data.zip`). If the dataset does not exist it will be downloaded and saved in this location._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\developer\\lib\\anaconda3\\lib\\site-packages (1.19.2)\n",
      "Requirement already satisfied: pandas in c:\\developer\\lib\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: wordcloud in c:\\developer\\lib\\anaconda3\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: pillow in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from wordcloud) (8.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from wordcloud) (3.3.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from wordcloud) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2020.6.20)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib->wordcloud) (1.15.0)\n",
      "Requirement already satisfied: contractions in c:\\developer\\lib\\anaconda3\\lib\\site-packages (0.0.48)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: anyascii in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.1.7)\n",
      "Requirement already satisfied: pyahocorasick in c:\\developer\\lib\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install wordcloud\n",
    "!{sys.executable} -m pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "\n",
    "from urllib.request import urlopen, Request\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import contractions\n",
    "\n",
    "from wordcloud import STOPWORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read File\n",
    "Read the file into memory to determine its overall shape. If the file does not exist locally it is downloaded into the `./resources` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCES_DIR = 'resources'\n",
    "file_name = 'raw_twitter_data.zip'\n",
    "file_path = './{}/{}'.format(RESOURCES_DIR, file_name)\n",
    "is_file_downloaded = os.path.exists(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_file_downloaded:\n",
    "    if not os.path.exists(resources_dir):\n",
    "        os.makedirs(resources_dir)\n",
    "    \n",
    "    file_url = 'https://dorman-public-data.s3-us-west-2.amazonaws.com/' + file_name\n",
    "    req = Request(file_url)\n",
    "    \n",
    "    with open(file_path, 'wb') as target:\n",
    "        target.write(urlopen(req).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "df = pd.read_csv(file_path, encoding = 'ISO-8859-1', names=col_names, compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def df_summary(data_frame):\n",
    "    BYTES_IN_MB = 1024**2\n",
    "    print('Columns: {}'.format(data_frame.columns))\n",
    "    print('Shape: {}'.format(data_frame.shape))\n",
    "    print('Memory Usage:{:.4f} Mb'.format(data_frame.memory_usage().sum() / BYTES_IN_MB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Extra Columns\n",
    "The only columns of real interest to train a model for tweet sentiment classification is the `target` and `text` columns. The `target` column indicates the sentiment of the tweet and `text` contains the text of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop(['id', 'date', 'flag', 'user'], axis='columns', inplace=True)\n",
    "df_summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis\n",
    "Examining the data for differences in data points associated with different sentiments can help us understand if our data is balanced across our target categories. Additionally, it can helps use understand if we should direct the focus of our training on to or away from any data characteristics to get the best possible sentiment classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "To determine how to clean the data we need to understand the extent of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cols = ['target', 'text']\n",
    "\n",
    "fig, axes = plt.subplots(ncols=1, figsize=(4, 2), dpi=100)\n",
    "\n",
    "sns.barplot(x=df[missing_cols].isnull().sum().index, y=df[missing_cols].isnull().sum().values, ax=axes)\n",
    "\n",
    "axes.set_ylabel('Number of Values', size=11, labelpad=20)\n",
    "axes.tick_params(axis='x', labelsize=10)\n",
    "axes.tick_params(axis='y', labelsize=10)\n",
    "\n",
    "axes.set_title('Missing Values in Data Set', fontsize=11)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucky for us, this data set appears to have no missing values to be deleted or replaced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Features\n",
    "Examining meta features can help determine the best way to clean the data and help narrow down what types of models will be most successful for predicting the target of a tweet based on its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_words'] = df['text'].apply(lambda s: len(s.split(' ')))\n",
    "df['num_unique_words'] = df['text'].apply(lambda s: len(set(s.split(' '))))\n",
    "df['num_stop_words'] = df['text'].apply(lambda s: len([w for w in s.lower().split(' ') if w in STOPWORDS]))\n",
    "df['avg_word_length'] = df['text'].apply(lambda s: np.mean([len(w) for w in s.split(' ')]))\n",
    "df['num_chars'] = df['text'].apply(lambda s: len(s))\n",
    "df['num_punctuation'] = df['text'].apply(lambda s: len([c for c in s if c in string.punctuation]))\n",
    "df['num_urls'] = df['text'].apply(lambda s: len([w for w in s.lower().split() if 'http' in w or 'https' in w]))\n",
    "df['num_hashtags'] = df['text'].apply(lambda s: len([c for c in s if c == '#']))\n",
    "df['num_mentions'] = df['text'].apply(lambda s: len([c for c in s if c == '@']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "meta_feature_cols = ['num_words', 'num_unique_words', 'num_stop_words', 'avg_word_length', \n",
    "                     'num_chars', 'num_punctuation', 'num_urls', 'num_hashtags', 'num_mentions']\n",
    "POSITIVE_TWEETS = df['target'] == 4\n",
    "\n",
    "figure, axes = plt.subplots(ncols=1, nrows=len(meta_feature_cols), figsize=(10,20), dpi=100)\n",
    "\n",
    "for i, meta_feature in enumerate(meta_feature_cols):\n",
    "    sns.histplot(df.loc[POSITIVE_TWEETS][meta_feature], kde=True, stat='density', label='Positive Tweets', ax=axes[i])\n",
    "    sns.histplot(df.loc[~POSITIVE_TWEETS][meta_feature], kde=True, stat='density', label='Negative Tweets', ax=axes[i], color=\"red\")\n",
    "    \n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].tick_params(axis='x', labelsize=8)\n",
    "    axes[i].tick_params(axis='y', labelsize=8)\n",
    "    axes[i].legend()\n",
    "    \n",
    "    axes[i].set_title('{} Target Distribution'.format(meta_feature), fontsize=8)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of meta features shows diversity among the targets in several features. Features such as `num_words`, `num_unique_words`, `num_punctuation`, `avg_word_length`, and `num_chars` have different distributions for positive and negative tweets. These features may be useful in our model. Features such as `num_stop_words` may also include valuable differences when it comes to predicting our target. Usually, this type of context data would be cleaned from tweets before training a standard model. [[1]](https://towardsdatascience.com/text-cleaning-methods-for-natural-language-processing-f2fc1796e8c7) As a result, some of these features will be left in and model selection will have to take the existence of the features in the data into account. Generally, NLP includes preprocessing to remove all stop words and stem remaining verbs. However, for this data set that information scrubbing could result in loss of data important for our model. To determine which stop words add no value to our target predictions we can analyze the n-grams for the data set. Starting with a pre-trained BERT model and fine tuning it to handle target prediction should be used. A BERT model is designed to be used with NLP data that includes more grammatical context as it has been pre-trained by Google on a large corpus that includes documents and books. [[2]](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html?m=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target distributions in the data set are perfectly balanced. This means we don't need to account for any imbalances in target distributions when training or validating our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_balance = df.groupby('target')['target'].agg('count').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=1, figsize=(4, 3), dpi=100)\n",
    "\n",
    "sns.countplot(x=df['target'], hue=df['target'], ax=axes)\n",
    "\n",
    "axes.set_ylabel('')\n",
    "axes.set_xticklabels(['Positive', 'Negative'])\n",
    "axes.tick_params(axis='x', labelsize=11)\n",
    "axes.tick_params(axis='y', labelsize=11)\n",
    "\n",
    "axes.set_title('Target Distribution', fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams\n",
    "To determine if any common words and phrases can be identified across our targets we can generate n-gram for analysis. This will help us identify if any unigrams or bigrams should be removed from the text due to their lack of distinction across positive and negative tweets. [[3]](https://albertauyeung.github.io/2018/06/03/generating-ngrams.html#:~:text=N%2Dgrams%20are%20contiguous%20sequences,appears%20in%20many%20different%20places.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(s, n=1):\n",
    "    tokens = [token for token in s.lower().split(' ') if token != '' if token not in STOPWORDS]\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]\n",
    "\n",
    "def graph_top_ngrams(df_neg_grams, df_pos_grams, n, n_gram):\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    sns.barplot(y=df_neg_grams[0].values[:n], x=df_neg_grams[1].values[:n], ax=axes[0], color='red')\n",
    "    sns.barplot(y=df_pos_grams[0].values[:n], x=df_pos_grams[1].values[:n], ax=axes[1], color='blue')\n",
    "\n",
    "    for i in range(2):\n",
    "        axes[i].spines['right'].set_visible(False)\n",
    "        axes[i].set_xlabel('')\n",
    "        axes[i].set_ylabel('')\n",
    "        axes[i].tick_params(axis='x', labelsize=13)\n",
    "        axes[i].tick_params(axis='y', labelsize=13)\n",
    "\n",
    "    axes[0].set_title('Top {} Common {}-Gram in Negative Tweets'.format(n, n_gram), fontsize=15)\n",
    "    axes[1].set_title('Top {} Common {}-Gram in Positive Tweets'.format(n, n_gram), fontsize=15)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_unigrams = defaultdict(int)\n",
    "pos_unigrams = defaultdict(int)\n",
    "\n",
    "for tweet in df[~POSITIVE_TWEETS]['text']:\n",
    "    for ngram in generate_ngrams(tweet):\n",
    "        neg_unigrams[ngram] += 1\n",
    "\n",
    "for tweet in df[POSITIVE_TWEETS]['text']:\n",
    "    for ngram in generate_ngrams(tweet):\n",
    "        pos_unigrams[ngram] += 1\n",
    "        \n",
    "df_neg_unigrams = pd.DataFrame(sorted(neg_unigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_pos_unigrams = pd.DataFrame(sorted(pos_unigrams.items(), key=lambda x: x[1])[::-1])\n",
    "\n",
    "graph_top_ngrams(df_neg_unigrams, df_pos_unigrams, 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_neg_uni = df_neg_unigrams.iloc[:25, 0]\n",
    "top_pos_uni = df_pos_unigrams.iloc[:25, 0]\n",
    "top_shared_uni = top_neg_uni[top_neg_uni.isin(top_pos_uni)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_bigrams = defaultdict(int)\n",
    "pos_bigrams = defaultdict(int)\n",
    "\n",
    "for tweet in df[~POSITIVE_TWEETS]['text']:\n",
    "    for ngram in generate_ngrams(tweet, 2):\n",
    "        neg_bigrams[ngram] += 1\n",
    "\n",
    "for tweet in df[POSITIVE_TWEETS]['text']:\n",
    "    for ngram in generate_ngrams(tweet, 2):\n",
    "        pos_bigrams[ngram] += 1\n",
    "\n",
    "df_neg_bigrams = pd.DataFrame(sorted(neg_bigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_pos_bigrams = pd.DataFrame(sorted(pos_bigrams.items(), key=lambda x: x[1])[::-1])\n",
    "\n",
    "graph_top_ngrams(df_neg_bigrams, df_pos_bigrams, 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_neg_bi = df_neg_bigrams.iloc[:50, 0]\n",
    "top_pos_bi = df_pos_bigrams.iloc[:50, 0]\n",
    "top_shared_bi = top_neg_bi[top_neg_bi.isin(top_pos_bi)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_trigrams = defaultdict(int)\n",
    "pos_trigrams = defaultdict(int)\n",
    "\n",
    "for tweet in df[~POSITIVE_TWEETS]['text']:\n",
    "    for ngram in generate_ngrams(tweet, 3):\n",
    "        neg_trigrams[ngram] += 1\n",
    "\n",
    "for tweet in df[POSITIVE_TWEETS]['text']:\n",
    "    for ngram in generate_ngrams(tweet, 3):\n",
    "        pos_trigrams[ngram] += 1\n",
    "\n",
    "df_neg_trigrams = pd.DataFrame(sorted(neg_trigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_pos_trigrams = pd.DataFrame(sorted(pos_trigrams.items(), key=lambda x: x[1])[::-1])\n",
    "\n",
    "graph_top_ngrams(df_neg_trigrams, df_pos_trigrams, 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_neg_tri = df_neg_trigrams.iloc[:50, 0]\n",
    "top_pos_tri = df_pos_trigrams.iloc[:50, 0]\n",
    "top_shared_tri = top_neg_tri[top_neg_tri.isin(top_pos_tri)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARED_NGRAMS = pd.Series(data=top_shared_tri)\n",
    "SHARED_NGRAMS = SHARED_NGRAMS.append(top_shared_bi)\n",
    "SHARED_NGRAMS = SHARED_NGRAMS.append(top_shared_uni)\n",
    "# Removing good or u could ruin context important to sentiment\n",
    "SHARED_NGRAMS = SHARED_NGRAMS[~SHARED_NGRAMS.str.contains(\"good\") & ~SHARED_NGRAMS.str.contains(\"u\")]\n",
    "SHARED_NGRAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels\n",
    "To simply our target representation we will convert it to a boolean label: `0=Positive Tweet; 1=Negative Tweet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['target'].apply(lambda i: 1 if i == 0 else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets\n",
    "To increase the models ability to predicte the targets the tweets are cleaned. This process involves taking a tweet through several steps of processing: `to_lowercase`, `remove_ngrams`, `expand_contractions`, `entity_to_char`, `remove_links`, `remove_mentions`, `replace_slang`, `remove_special_chars`, `remove_puncuation`, and `trim_spaces`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(s):\n",
    "    return s.lower()\n",
    "\n",
    "def remove_ngrams(s):\n",
    "    for ngram in SHARED_NGRAMS:\n",
    "        result = s.replace(ngram, '')\n",
    "    return result\n",
    "\n",
    "def expand_contractions(s):\n",
    "    expanded = []\n",
    "    for word in s.split():\n",
    "        expanded.append(contractions.fix(word))\n",
    "    return ' '.join(expanded)\n",
    "\n",
    "def entity_to_char(s):\n",
    "    result = re.sub(r'&gt;', '>', s)\n",
    "    result = re.sub(r'&lt;', '<', result)\n",
    "    result = re.sub(r'&amp;', '&', result)\n",
    "    return result\n",
    "\n",
    "def remove_links(s):\n",
    "    return re.sub(r'https?://[A-Za-z0-9./]+', '', s)\n",
    "\n",
    "def remove_mentions(s):\n",
    "    return re.sub(r'(@[A-Za-z0-9_]+)', '', s)\n",
    "\n",
    "def replace_slang(s):\n",
    "    result = re.sub(r'w/e', 'whatever', s)\n",
    "    result = re.sub(r'\\bu\\b', 'you', s)\n",
    "    result = re.sub(r'w/', 'with', result)\n",
    "    result = re.sub(r'<3', 'love', result)\n",
    "    return result\n",
    "\n",
    "def remove_special_chars(s):\n",
    "    result = re.sub(r\"\\x89Û_\", \"\", s)\n",
    "    result = re.sub(r\"\\x89ÛÒ\", \"\", result)\n",
    "    result = re.sub(r\"\\x89ÛÓ\", \"\", result)\n",
    "    result = re.sub(r\"\\x89ÛÏ\", \"\", result)\n",
    "    result = re.sub(r\"\\x89Û÷\", \"\", result)\n",
    "    result = re.sub(r\"\\x89Ûª\", \"\", result)\n",
    "    result = re.sub(r\"\\x89Û\\x9d\", \"\", result)\n",
    "    result = re.sub(r\"å_\", \"\", result)\n",
    "    result = re.sub(r\"\\x89Û¢\", \"\", result)\n",
    "    result = re.sub(r\"\\x89Û¢åÊ\", \"\", result)\n",
    "    result = re.sub(r\"åÊ\", \"\", result)\n",
    "    result = re.sub(r\"åÈ\", \"\", result)  \n",
    "    result = re.sub(r\"Ì©\", \"e\", result)\n",
    "    result = re.sub(r\"å¨\", \"\", result)\n",
    "    result = re.sub(r\"åÇ\", \"\", result)\n",
    "    result = re.sub(r\"åÀ\", \"\", result)\n",
    "    return result\n",
    "\n",
    "def remove_puncuation(s):\n",
    "    result = s.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "    result = re.sub('[0-9]+', '', result)\n",
    "    return result\n",
    "\n",
    "def trim_spaces(s):\n",
    "    result = re.sub('\\t', ' ',  s)\n",
    "    result = re.sub(' +', ' ', result)\n",
    "    return result.strip()\n",
    "\n",
    "def clean(s):\n",
    "    result = to_lowercase(s)\n",
    "    result = remove_ngrams(result)\n",
    "    result = expand_contractions(result)\n",
    "    result = entity_to_char(result)\n",
    "    result = remove_links(result)\n",
    "    result = remove_mentions(result)\n",
    "    result = replace_slang(result)\n",
    "    result = remove_special_chars(result)\n",
    "    result = remove_puncuation(result)\n",
    "    result = trim_spaces(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['text'].apply(lambda s: clean(s))\n",
    "df['tweet'].head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Set: Training, Testing, Validation\n",
    "The data is now ready for pre-processing where it is converted into number representations that can be used for model training. To ensure the final model can be tested after and validated during training on unique tweet samples, the data is split into 3 set. The data will be split 80:20 to create a training set and a testing/validation set. The testing/validation set is then be split 80:20 to create the training set and the validation set. These data set will then be saved into 3 files compressed in zip archives in the `./resources` directory for training=`train_twitter_data.zip`, testing=`test_twitter_data.zip`, and validation=`validate_twitter_data.zip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['target', 'tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=1337)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.8, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELIMITER = '=' * 5\n",
    "print(DELIMITER + 'TRAIN' + DELIMITER)\n",
    "df_summary(df_train)\n",
    "print(DELIMITER + 'TEST' + DELIMITER)\n",
    "df_summary(df_test)\n",
    "print(DELIMITER + 'VALIDATE' + DELIMITER)\n",
    "df_summary(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('./{}/{}'.format(RESOURCES_DIR, 'train_twitter_data.zip'), compression=dict(method='zip', archive_name='train_twitter_data.csv'), index=False)\n",
    "df_test.to_csv('./{}/{}'.format(RESOURCES_DIR, 'test_twitter_data.zip'), compression=dict(method='zip', archive_name='test_twitter_data.csv'), index=False)\n",
    "df_val.to_csv('./{}/{}'.format(RESOURCES_DIR, 'validate_twitter_data.zip'), compression=dict(method='zip', archive_name='validate_twitter_data.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
