{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_automl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDVcSUQHh1bs"
      },
      "source": [
        "# Create Google Cloud Resources and Train with AutoML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TosYwamtiAW_"
      },
      "source": [
        "## Google Cloud Setup\n",
        "1. To train a twitter sentiment classification model with Google's AutoML cloud solution you must create a Google Cloud account [here](cloud.google.com)\n",
        "\n",
        "2. In the Google Cloud console create a new Google Cloud project.\n",
        "\n",
        "3. Confirm that billing is enabled for you Google Cloud project - [learn more](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
        "\n",
        "4. Enable the AutoML and Cloud Storage APIs [here](https://console.cloud.google.com/flows/enableapi?apiid=storage-component.googleapis.com,automl.googleapis.com,storage-api.googleapis.com&redirect=https://console.cloud.google.com&_ga=2.19444408.1477944611.1615487721-641531934.1615487721)\n",
        "\n",
        "5. Create a service account with the roles `AutoML Admin` and `AutoML Service Agent` and download a key file for it [here](https://cloud.google.com/iam/docs/creating-managing-service-accounts#creating_a_service_account)\n",
        "\n",
        "6. Get the [Project ID](https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects) for your Cloud project\n",
        "\n",
        "7. Create a [Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) to import and store data in for model training\n",
        "\n",
        "8. Follow steps in this notebook\n",
        "\n",
        "---\n",
        "\n",
        "__Resources__\n",
        "\n",
        "\n",
        "* https://cloud.google.com/natural-language/automl/docs/how-to   \n",
        "\n",
        "* https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/notebooks/samples/tables/census_income_prediction/getting_started_notebook.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtKUKKQij6lC"
      },
      "source": [
        "## Setup and Authenticate\n",
        "Fill in the necessary account information to begin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rln6V3_-5G_a"
      },
      "source": [
        "!pip install --upgrade --quiet --user google-cloud-automl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LarVwkLLkJuc"
      },
      "source": [
        "import sys\n",
        "\n",
        "from google.cloud import automl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QztTYl6r5OFM"
      },
      "source": [
        "PROJECT_ID = '' #@param {type:\"string\"}\n",
        "DISPLAY_NAME = '' #@param {type:\"string\"}\n",
        "BUCKET_NAME = '' #@param {type:\"string\"}\n",
        "COMPUTE_REGION = 'us-central1' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unOAn3fD4919"
      },
      "source": [
        "if 'google.colab' in sys.modules:    \n",
        "  from google.colab import files\n",
        "  keyfile_upload = files.upload()\n",
        "  keyfile = list(keyfile_upload.keys())[0]\n",
        "  %env GOOGLE_APPLICATION_CREDENTIALS $keyfile\n",
        "  !gcloud auth activate-service-account --key-file $keyfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwuCXf515nWv"
      },
      "source": [
        "!gsutil ls -al gs://$BUCKET_NAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImKKrzev5zjQ"
      },
      "source": [
        "## Create Dataset and Import Data\n",
        "Make sure the variable values are set correctly for your purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbuq92Zy5Rvu"
      },
      "source": [
        "# A name for the AutoML tables Dataset to create.\n",
        "DATASET_DISPLAY_NAME = 'twitter_data' #@param {type: 'string'}\n",
        "# The GCS data to import data from (doesn't need to exist).\n",
        "INPUT_CSV_NAME = 'clean_twitter_data.csv' #@param {type: 'string'}\n",
        "# A name for the AutoML tables model to create.\n",
        "MODEL_DISPLAY_NAME = 'twitter_sentiment_model' #@param {type: 'string'}\n",
        "\n",
        "assert all([\n",
        "    PROJECT_ID,\n",
        "    COMPUTE_REGION,\n",
        "    DATASET_DISPLAY_NAME,\n",
        "    INPUT_CSV_NAME,\n",
        "    MODEL_DISPLAY_NAME,\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FivIBKtC7pxy"
      },
      "source": [
        "## Import Training Data\n",
        "First we create the dataset in Google Cloud. Then we populate it with our data. Lastly, we test our import be reading the dataset from Google Cloud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkcjWv7L7lhy"
      },
      "source": [
        "client = automl.AutoMlClient()\n",
        "project_location = 'projects/{}/locations/{}'.format(PROJECT_ID, COMPUTE_REGION)\n",
        "\n",
        "metadata = automl.TextSentimentDatasetMetadata(\n",
        "    sentiment_max=1\n",
        ") \n",
        "\n",
        "dataset = automl.Dataset(\n",
        "    display_name=DATASET_DISPLAY_NAME, text_sentiment_dataset_metadata=metadata\n",
        ")\n",
        "\n",
        "response = client.create_dataset(parent=project_location, dataset=dataset)\n",
        "created_dataset = response.result()\n",
        "dataset_id = created_dataset.name.split('/')[-1]\n",
        "\n",
        "print('Dataset name: {}'.format(created_dataset.name))\n",
        "print('Dataset id: {}'.format(dataset_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO501CWH8S-N"
      },
      "source": [
        "gcs_dataset_uri = 'gs://{}/{}.csv'.format(BUCKET_NAME, INPUT_CSV_NAME)\n",
        "\n",
        "if 'google.colab' in sys.modules:    \n",
        "  from google.colab import files\n",
        "  dataset_upload = files.upload()\n",
        "  dataset_csv = list(dataset_upload.keys())[0]\n",
        "\n",
        "!gsutil ls gs://$BUCKET_NAME || gsutil mb -l $COMPUTE_REGION gs://$BUCKET_NAME\n",
        "!gsutil cp $dataset_csv $gcs_dataset_uri"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0D58lTb82bl"
      },
      "source": [
        "dataset_full_id = client.dataset_path(PROJECT_ID, COMPUTE_REGION, dataset_id)\n",
        "\n",
        "input_uris = gcs_dataset_uri.split(\",\")\n",
        "gcs_source = automl.GcsSource(input_uris=input_uris)\n",
        "input_config = automl.InputConfig(gcs_source=gcs_source)\n",
        "\n",
        "response = client.import_data(name=dataset_full_id, input_config=input_config)\n",
        "\n",
        "print('Processing import...')\n",
        "print('Data imported. {}'.format(response.result()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysKAKPuc9UWm"
      },
      "source": [
        "## Train Model\n",
        "Once the dataset has imported we can start the training job on Google Cloud. This takes several hours. You will be email when it completes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqbz0EGs9Wft"
      },
      "source": [
        "metadata = automl.TextSentimentModelMetadata()\n",
        "model = automl.Model(\n",
        "    display_name=MODEL_DISPLAY_NAME,\n",
        "    dataset_id=dataset_id,\n",
        "    text_sentiment_model_metadata=metadata,\n",
        ")\n",
        "\n",
        "response = client.create_model(parent=project_location, model=model)\n",
        "\n",
        "print('Training operation name: {}'.format(response.operation.name))\n",
        "print('Training started...')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTg4qYDlZ9gT"
      },
      "source": [
        "Confirm your model finished training and is accessible via GCS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzDIfA5-Z34D"
      },
      "source": [
        "created_model = response.result()\n",
        "print('Training finished. {}'.format(created_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMqiJ8kVZaJa"
      },
      "source": [
        "## Deploy Model\n",
        "Deploy the model once training has completed. Once the model is deployed verify its deployment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvR4vmX2ZZnv"
      },
      "source": [
        "response = client.deploy_model(name=created_model.name)\n",
        "print(f\"Model deployment finished. {response.result()}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}