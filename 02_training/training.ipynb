{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi\n",
    "# https://medium.com/analytics-vidhya/install-tensorflow-gpu-2-4-0-with-cuda-11-0-and-cudnn-8-using-anaconda-8c6472c9653f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing and Training the Model\n",
    "This notebook goes through the steps in turning the cleaned data into a representation that can be used to train a pre-trained BERT model for sentiment analysis.\n",
    "\n",
    "__Columns__\n",
    "- `target` sentiment of tweet (0=positive, 1=negative)\n",
    "- `tweet` cleaned tweet text\n",
    "\n",
    "The processes used in this notebook were inspired by the [this tutorial](https://www.tensorflow.org/tutorials/text/classify_text_with_bert) in the TensorFlow Docs.\n",
    "\n",
    "_Note:_\n",
    "_This notebook expects the zipped datasets to be in the ./resources sub-directory with the names train_twitter_data.zip, test_twitter_data, and validate_twitter_data (e.g., ./resources/train_twitter_data.zip). If the datasets do not exist open and run the `cleaning.ipynb` notebook to generate them._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-nightly in c:\\users\\ryand\\appdata\\roaming\\python\\python38\\site-packages (2.5.0.dev20210310)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly) (3.15.5)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly) (0.10.0)\n",
      "Requirement already satisfied: tf-estimator-nightly~=2.5.0.dev in c:\\users\\ryand\\appdata\\roaming\\python\\python38\\site-packages (from tf-nightly) (2.5.0.dev2021031001)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly) (3.7.4.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly) (1.12)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\ryand\\appdata\\roaming\\python\\python38\\site-packages (from tf-nightly) (3.1.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly) (1.12.1)\n",
      "Requirement already satisfied: tb-nightly~=2.5.0.a in c:\\users\\ryand\\appdata\\roaming\\python\\python38\\site-packages (from tf-nightly) (2.5.0a20210310)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly) (0.36.2)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly) (1.6.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly) (1.1.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly) (1.19.5)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly) (1.1.2)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\ryand\\appdata\\roaming\\python\\python38\\site-packages (from tf-nightly) (0.4.0)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\ryand\\appdata\\roaming\\python\\python38\\site-packages (from tf-nightly) (1.34.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tb-nightly~=2.5.0.a->tf-nightly) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.8.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tb-nightly~=2.5.0.a->tf-nightly) (52.0.0.post20210125)\n",
      "Requirement already satisfied: tensorboard-data-server<0.4.0,>=0.3.0 in c:\\users\\ryand\\appdata\\roaming\\python\\python38\\site-packages (from tb-nightly~=2.5.0.a->tf-nightly) (0.3.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tb-nightly~=2.5.0.a->tf-nightly) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tb-nightly~=2.5.0.a->tf-nightly) (0.4.3)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.27.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly) (3.1.0)\n",
      "Requirement already satisfied: tensorflow-text-nightly in c:\\users\\ryand\\appdata\\roaming\\python\\python38\\site-packages (2.5.0.dev20210305)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tensorflow-text-nightly) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tensorflow-hub>=0.8.0->tensorflow-text-nightly) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tensorflow-hub>=0.8.0->tensorflow-text-nightly) (3.15.5)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from protobuf>=3.8.0->tensorflow-hub>=0.8.0->tensorflow-text-nightly) (1.15.0)\n",
      "Requirement already satisfied: tf-models-nightly in c:\\users\\ryand\\appdata\\roaming\\python\\python38\\site-packages (2.4.0.dev20210310)\n",
      "Requirement already satisfied: pycocotools in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (2.0.2)\n",
      "Requirement already satisfied: tf-slim>=1.1.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-datasets in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (1.6.1)\n",
      "Requirement already satisfied: tensorflow-hub>=0.6.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (0.11.0)\n",
      "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (0.5.0)\n",
      "Requirement already satisfied: Cython in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (0.29.22)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (1.19.5)\n",
      "Requirement already satisfied: six in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (1.15.0)\n",
      "Requirement already satisfied: sacrebleu in c:\\users\\ryand\\appdata\\roaming\\python\\python38\\site-packages (from tf-models-nightly) (1.5.1)\n",
      "Requirement already satisfied: py-cpuinfo>=3.3.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (7.0.0)\n",
      "Requirement already satisfied: psutil>=5.4.3 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (5.8.0)\n",
      "Requirement already satisfied: kaggle>=1.3.9 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (1.5.10)\n",
      "Requirement already satisfied: tensorflow-addons in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (0.12.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (3.3.4)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (0.1.95)\n",
      "Requirement already satisfied: pandas>=0.22.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (1.2.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (5.4.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (8.1.2)\n",
      "Requirement already satisfied: tensorflow-text-nightly in c:\\users\\ryand\\appdata\\roaming\\python\\python38\\site-packages (from tf-models-nightly) (2.5.0.dev20210305)\n",
      "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (2.11.0)\n",
      "Requirement already satisfied: gin-config in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (0.4.0)\n",
      "Requirement already satisfied: tf-nightly in c:\\users\\ryand\\appdata\\roaming\\python\\python38\\site-packages (from tf-models-nightly) (2.5.0.dev20210310)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (2.0.2)\n",
      "Requirement already satisfied: opencv-python-headless in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (4.5.1.48)\n",
      "Requirement already satisfied: seqeval in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (1.2.2)\n",
      "Requirement already satisfied: oauth2client in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-models-nightly) (4.1.3)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (0.19.0)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (1.26.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (0.1.0)\n",
      "Requirement already satisfied: google-auth<2dev,>=1.16.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (1.27.1)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (3.0.1)\n",
      "Requirement already satisfied: pytz in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-nightly) (2021.1)\n",
      "Requirement already satisfied: packaging>=14.3 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-nightly) (20.9)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-nightly) (2.25.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-nightly) (3.15.5)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-nightly) (1.53.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-nightly) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-nightly) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-nightly) (4.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-nightly) (4.7.2)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-cloud-bigquery>=0.31.0->tf-models-nightly) (1.6.0)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-cloud-bigquery>=0.31.0->tf-models-nightly) (1.14.2)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-cloud-bigquery>=0.31.0->tf-models-nightly) (1.2.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in c:\\users\\ryand\\appdata\\roaming\\python\\python38\\site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-nightly) (1.34.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-nightly) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-nightly) (1.14.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-nightly) (2.20)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.6.7->tf-models-nightly) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from kaggle>=1.3.9->tf-models-nightly) (2.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from kaggle>=1.3.9->tf-models-nightly) (4.59.0)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from kaggle>=1.3.9->tf-models-nightly) (4.0.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from kaggle>=1.3.9->tf-models-nightly) (2020.12.5)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from kaggle>=1.3.9->tf-models-nightly) (1.26.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-nightly) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-nightly) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-nightly) (2.10)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-nightly) (0.1.5)\n",
      "Requirement already satisfied: absl-py>=0.2.2 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-slim>=1.1.0->tf-models-nightly) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from matplotlib->tf-models-nightly) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from matplotlib->tf-models-nightly) (0.10.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from python-slugify->kaggle>=1.3.9->tf-models-nightly) (1.3)\n",
      "Requirement already satisfied: portalocker==2.0.0 in c:\\users\\ryand\\appdata\\roaming\\python\\python38\\site-packages (from sacrebleu->tf-models-nightly) (2.0.0)\n",
      "Requirement already satisfied: pywin32!=226 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from portalocker==2.0.0->sacrebleu->tf-models-nightly) (227)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from seqeval->tf-models-nightly) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-nightly) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-nightly) (1.0.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tensorflow-addons->tf-models-nightly) (2.11.1)\n",
      "Requirement already satisfied: future in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tensorflow-datasets->tf-models-nightly) (0.18.2)\n",
      "Requirement already satisfied: dill in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tensorflow-datasets->tf-models-nightly) (0.3.3)\n",
      "Requirement already satisfied: termcolor in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tensorflow-datasets->tf-models-nightly) (1.1.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tensorflow-datasets->tf-models-nightly) (20.3.0)\n",
      "Requirement already satisfied: promise in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tensorflow-datasets->tf-models-nightly) (2.3)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tensorflow-datasets->tf-models-nightly) (0.28.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tensorflow-datasets->tf-models-nightly) (5.1.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly->tf-models-nightly) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly->tf-models-nightly) (3.7.4.3)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\ryand\\appdata\\roaming\\python\\python38\\site-packages (from tf-nightly->tf-models-nightly) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly->tf-models-nightly) (1.1.2)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly->tf-models-nightly) (0.36.2)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly->tf-models-nightly) (1.12.1)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\ryand\\appdata\\roaming\\python\\python38\\site-packages (from tf-nightly->tf-models-nightly) (0.4.0)\n",
      "Requirement already satisfied: tb-nightly~=2.5.0.a in c:\\users\\ryand\\appdata\\roaming\\python\\python38\\site-packages (from tf-nightly->tf-models-nightly) (2.5.0a20210310)\n",
      "Requirement already satisfied: tf-estimator-nightly~=2.5.0.dev in c:\\users\\ryand\\appdata\\roaming\\python\\python38\\site-packages (from tf-nightly->tf-models-nightly) (2.5.0.dev2021031001)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly->tf-models-nightly) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly->tf-models-nightly) (1.12)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tf-nightly->tf-models-nightly) (1.6.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (0.4.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.4.0,>=0.3.0 in c:\\users\\ryand\\appdata\\roaming\\python\\python38\\site-packages (from tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (0.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (3.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages (from pydot) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "!{sys.executable} -m pip install tf-nightly --user\n",
    "!{sys.executable} -m pip install tensorflow-text-nightly --user\n",
    "!{sys.executable} -m pip install tf-models-nightly --user\n",
    "!{sys.executable} -m pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryand\\anaconda3\\envs\\tf-nlp\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:37: UserWarning: You are currently using a nightly version of TensorFlow (2.5.0-dev20210310). \n",
      "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
      "If you encounter a bug, do not file an issue on GitHub.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n",
    "from official.nlp import optimization\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.set_visible_devices([], 'GPU')\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files\n",
    "Read the files into memory and convert them to TensorFlow datasets to begin pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCES_PATH = './resources/{}'\n",
    "BATCH_SIZE=16\n",
    "\n",
    "def twitter_data_to_dataset(filename):\n",
    "    pd_df = pd.read_csv(RESOURCES_PATH.format(filename), encoding = 'ISO-8859-1', compression='zip')\n",
    "    tweets =  tf.constant(pd_df['tweet'].to_numpy(dtype='U'), dtype=tf.string)\n",
    "    targets = tf.constant(pd_df['target'].to_numpy(dtype='i4'), dtype=tf.int32)\n",
    "    raw_ds = tf.data.Dataset.from_tensor_slices((tweets, targets))\n",
    "    return raw_ds.batch(BATCH_SIZE, drop_remainder=False).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = twitter_data_to_dataset('train_twitter_data.zip')\n",
    "test_ds = twitter_data_to_dataset('test_twitter_data.zip')\n",
    "val_ds = twitter_data_to_dataset('validate_twitter_data.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: b'i hate having a cold'\n",
      "Label : 1\n",
      "Review: b'oh that is weird'\n",
      "Label : 1\n",
      "Review: b'just flew back to china and was saddened to discover that blogspot and blogger was blogged can not blog now'\n",
      "Label : 1\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in range(3):\n",
    "        print(f'Review: {text_batch.numpy()[i]}')\n",
    "        print(f'Label : {label_batch.numpy()[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BERT Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Preprocess model     : https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
    "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "\n",
    "print(f'BERT model           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model     : {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_mask', 'input_type_ids', 'input_word_ids']\n",
      "Shape      : (1, 128)\n",
      "Word Ids   : [ 101 1996 4031 2001 2026 5440  102    0    0    0    0    0]\n",
      "Input Mask : [1 1 1 1 1 1 1 0 0 0 0 0]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "text_test = ['the product was my favorite']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Pooled Outputs Shape:(1, 512)\n",
      "Pooled Outputs Values:[ 0.6691764   0.58543855 -0.0314561   0.48074463 -0.25392526  0.8787984\n",
      "  0.9961011  -0.9899798   0.09613888 -0.8066808  -0.01391775 -0.99810755]\n",
      "Sequence Outputs Shape:(1, 128, 512)\n",
      "Sequence Outputs Values:[[-0.01279837  0.9729931   0.32275626 ... -0.76048213  1.4174023\n",
      "  -0.57385325]\n",
      " [-0.1144598   0.30194023 -1.2570345  ... -0.32878464  0.74750984\n",
      "   0.2168648 ]\n",
      " [-0.91389835  0.1244075   0.7862899  ...  0.5537697   1.3620057\n",
      "  -0.32195392]\n",
      " ...\n",
      " [-0.15185931 -0.08935617  0.14648202 ...  0.1457319   0.95670724\n",
      "   0.15815215]\n",
      " [-0.06271622 -0.01314841  0.16347808 ... -0.02113535  0.90248054\n",
      "  -0.17281137]\n",
      " [ 0.01646246  0.21026973  0.17396608 ... -0.1056128   0.9892045\n",
      "  -0.3466006 ]]\n"
     ]
    }
   ],
   "source": [
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "A fine-tuned model for tweet sentiment analysis will be constructed using the input layer, a pre-processing model, the uncased BERT model, one Dense layer, and a Dropout layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.24863514]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(classifier_model, RESOURCES_PATH.format('model.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "Tweet sentiment classification between positive and negative is actually a binary classification problem at its core. It outputs a singlue-unit layer in the form of a probability making binary crossentropy the function of choice to measure loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "Use the same optimizer BERT was trained with the Adam (Adaptive Moments) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * EPOCHS\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Train\n",
    "Now that `loss`, `metrics`, and our `optimizer` are defined we can compile our model and begin training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "55/80 [===================>..........] - ETA: 1:17 - loss: 0.4128 - binary_accuracy: 0.6442"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[16,8,128,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node model/BERT_encoder/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/bert_encoder/StatefulPartitionedCall/transformer/layer_2/self_attention/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_65232]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-77a3b138e404>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training model with {tfhub_handle_encoder}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m history = classifier_model.fit(x=train_ds,\n\u001b[0m\u001b[0;32m      3\u001b[0m                                \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                epochs=EPOCHS)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1155\u001b[0m                 _r=1):\n\u001b[0;32m   1156\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1157\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1158\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3016\u001b[0m       (graph_function,\n\u001b[0;32m   3017\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3018\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3019\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[16,8,128,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node model/BERT_encoder/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/bert_encoder/StatefulPartitionedCall/transformer/layer_2/self_attention/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_65232]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = classifier_model.fit(x=train_ds,\n",
    "                               validation_data=val_ds,\n",
    "                               epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'twitter_sentiment'\n",
    "saved_model_path = './resources/{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "\n",
    "classifier_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload Model and Test Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_my_examples(inputs, results):\n",
    "    result_for_printing = [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}' for i in range(len(inputs))]\n",
    "    print(*result_for_printing, sep='\\n')\n",
    "    print()\n",
    "\n",
    "\n",
    "examples = [\n",
    "    'the product was my favorite',  # this is the same sentence tried earlier\n",
    "    'the product was so great',\n",
    "    'the product was meh',\n",
    "    'the products was okish',\n",
    "    'the product was terrible'\n",
    "]\n",
    "\n",
    "reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n",
    "original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n",
    "\n",
    "print('Results from the saved model:')\n",
    "print_my_examples(examples, reloaded_results)\n",
    "\n",
    "print('Results from the model in memory:')\n",
    "print_my_examples(examples, original_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with TF Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_results = reloaded_model.signatures['serving_default'](tf.constant(examples))\n",
    "serving_results = tf.sigmoid(serving_results['classifier'])\n",
    "\n",
    "print_my_examples(examples, serving_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
